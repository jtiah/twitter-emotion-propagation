{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Propagation in Online Social Networks\n",
    "##### Code Workflow\n",
    "This is the framework code for the '*Emotion Propagation in Online Social Networks*' project. <br>\n",
    "You can find the associated paper here."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection and Pre-Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom modules path\n",
    "import sys; sys.path.append('./libraries')\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "# Custom modules\n",
    "from utils import *\n",
    "# Basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from datetime import timedelta\n",
    "import re\n",
    "# Network libraries\n",
    "import networkx as nx\n",
    "from scipy.sparse import csgraph\n",
    "import network_distance as nd\n",
    "# Statistic libraries\n",
    "from scipy.stats import entropy, pearsonr, spearmanr, ks_2samp, kruskal, tukey_hsd\n",
    "# NLP libraries\n",
    "import tweetnlp\n",
    "from nltk.corpus import stopwords\n",
    "# Plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filepaths & Constants"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relative paths to the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '../data/'\n",
    "raw_path = root + '0- Raw collected data/'\n",
    "processed_path = root + '1- Processed data/'\n",
    "finetuning_path = root + '2- NLP/1- Fine Tuning'\n",
    "emotions_path = root + '2- NLP/2- Emotions/'\n",
    "propagation_path = root + '3- Propagation/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Switches to process data from scratch, or load the processed data. <br>\n",
    "For looking at results, suffices to set all switches to False. <br>\n",
    "For corroboration of the steps, suffices to set the relevant switches to True. <br>\n",
    "For complete replication of the results from the raw collected data, set everything to True. NOTE: We do not recommend this, since the classification process takes a very long time. We therefore recommend running the classification in a High Perfomance Cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SWITCHES : \n",
    "process = False                 # To process everything from Raw Data or load Processed Data\n",
    "classify = False                 # To classify and extract tweets emotions or load Classified Data\n",
    "propagate = False                # To calculate propagation or load Propagation Data\n",
    "access_to_all = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few variable settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTINGS\n",
    "# Set of Networks\n",
    "networks = {\n",
    "    \"us_congressmen\": {}, \n",
    "    \"journalists\": {}, \n",
    "    \"universities\": {}\n",
    "    }\n",
    "\n",
    "# Set of emotions (and their associated color)\n",
    "emotions = {\n",
    "    \"joy\": \"gold\",\n",
    "    \"optimism\": \"lawngreen\",\n",
    "    \"anger\": \"crimson\",\n",
    "    \"sadness\": \"mediumslateblue\"\n",
    "    }\n",
    "\n",
    "# Set of Datasets collected\n",
    "datasets = [\"user_info\", \"tweets\", \"follows\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load raw data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data was collected using [Twitter Lists](https://help.twitter.com/en/using-twitter/twitter-lists). This allowed us to collect data on specific users from a determined \"community\". The collected \"Twitter Communities\" are from three different domains:\n",
    "- US Congressmen: Current and former (previous period) US Congressmen (Senators and Representatives);\n",
    "- Journalists:  Journalists and staff from Wall Street Journal, NY Times, NY Post, Washington Post and LA Times;\n",
    "- Universities: Researchers and staff from Cambridge University, NY University and Princeton University  \n",
    "\n",
    "For each of those communities, three datasets were collected:\n",
    "- User Info: includes Twitter User ID, Display Name, Username (handle), Public Metrics, etc;\n",
    "- Tweets: includes tweets for the period Jan-2022 to Feb-2023 for each user\n",
    "- Follows: the list of people following by each of the users in User Info. This is used to construct the network\n",
    "\n",
    "We'll organize our networks in a dictionary containing each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if access_to_all:\n",
    "    print(\">>> LOADED DATA <<<\")\n",
    "    for net in networks:\n",
    "\n",
    "        if not process:\n",
    "            # We don't save 'follows' as processed\n",
    "            datasets = datasets[:2]\n",
    "            path = processed_path\n",
    "\n",
    "        else:\n",
    "            # If we process everything from raw\n",
    "            path = raw_path + f\"{net}/\"\n",
    "\n",
    "        for ds in datasets:\n",
    "\n",
    "            networks[net][ds] = pd.read_csv(path + f\"DF-{net}-{ds}.csv\", encoding=\"utf-8\", dtype={\"id\": str, \"author_id\": str, \"mainUserID\": str, \n",
    "                                                \"retweeted\": str, \"possibly_sensitive\": str})\n",
    "\n",
    "            # Filter only tweets from before 23/12\n",
    "            if ds == \"tweets\":\n",
    "                #networks[net][ds].loc[:,\"created_at\"] = pd.to_datetime(networks[net][ds].created_at).dt.date\n",
    "                # We need to convert to Datetime format to truncate miliseconds\n",
    "                networks[net][ds].loc[:,\"created_at\"] = pd.to_datetime(networks[net][ds].created_at, format=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "            print(f\"{'RAW' if process else 'PROCESSED'} DATA // Network: {net} - {ds} - Shape: {networks[net][ds].shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Process Tweets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace Retweets' text by original text\n",
    "By default, when a Tweet is a Retweet, Twitter appends the prefix RT and shortens the text (possibly to save space). Nevertheless, the id of the original tweet is also returned. From this ID we restore the original text of the tweet and replace the shorten version by the original one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if process:\n",
    "    \n",
    "    for net in networks:\n",
    "\n",
    "        # Read auxiliary retweet data and rename columns to match Tweets DF\n",
    "        aux_rt = pd.read_csv(raw_path + f\"{net}/DF-{net}-aux_retweets.csv\", encoding=\"utf-8\", dtype={\"id\": str})\n",
    "        aux_rt.columns = [\"retweeted\", \"text\"]\n",
    "\n",
    "        # Split Tweets DF in original tweets and retweets\n",
    "        originals = networks[net][\"tweets\"][networks[net][\"tweets\"].retweeted.isna()]\n",
    "        retweets = networks[net][\"tweets\"][~networks[net][\"tweets\"].retweeted.isna()]\n",
    "\n",
    "        # Merge the retweets with the aux data to get the complete text\n",
    "        retweets = retweets.merge(aux_rt, on= \"retweeted\", how=\"inner\") # If inner, we are discarding RT that haven't been found\n",
    "\n",
    "        # Replace the RT shorten text for the original text, drop aux col, and rename back to 'text'\n",
    "        retweets[\"text_x\"] = retweets.text_y\n",
    "        retweets.drop(\"text_y\", axis=1, inplace=True)\n",
    "        retweets.rename(columns={'text_x':'text'}, inplace=True)\n",
    "\n",
    "        # Put back together the Tweets Dataset\n",
    "        networks[net][\"tweets\"] = pd.concat([originals, retweets])\n",
    "        networks[net][\"tweets\"].sort_index(inplace=True, ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter Tweets\n",
    "\n",
    "The data-cleaning process of the tweets include:\n",
    "- Filter out non-English tweets: The NLP model used is trained only on English tweets, to preserve the quality of the analysis;\n",
    "- Remove mentions: Mentions in Tweets are removed, to reduce noise in the text;\n",
    "- Remove URLs: URLs are also removed, for the same reasons as above;\n",
    "- Remove undefined characters: Some tweets contain undefined non-parsed characters that were removed to preserve the quality of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if process:\n",
    "    \n",
    "    for net in networks:\n",
    "\n",
    "        # Filter our non-English tweets\n",
    "        networks[net][\"tweets\"] = tweetFilter(networks[net][\"tweets\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check overall yearly activity for different window sizes, using the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if process:\n",
    "\n",
    "    # Style\n",
    "    windows = {\n",
    "        3: [\"dotted\", \".\"],\n",
    "        7: [\"dashed\", \"^\"],\n",
    "        10: [\"solid\", \"o\"]\n",
    "    }\n",
    "    \n",
    "    # Check for different window sizes\n",
    "    for w in windows:\n",
    "        \n",
    "        # Calculate active users\n",
    "        for net in networks:\n",
    "\n",
    "            # We need to check if the window will leave the last gap unfinished\n",
    "            temp = networks[net][\"tweets\"].loc[:,[\"created_at\", \"author_id\"]].groupby(pd.Grouper(key=\"created_at\", freq=f\"{w}d\")).nunique(\"author_id\").reset_index()\n",
    "            if ((networks[net][\"tweets\"].created_at.max() - pd.to_datetime(\"2022-01-01\", format=\"%Y-%m-%d %H:%M:%S\")).days + 1) % w > 0:\n",
    "                temp = temp.iloc[:-1]\n",
    "            temp[\"author_id\"] = temp[\"author_id\"] / networks[net][\"tweets\"].author_id.nunique() * 100\n",
    "            sns.set(rc={'figure.figsize':(16,4)})\n",
    "            sns.set_style(\"ticks\")\n",
    "            sns.lineplot(temp, x=\"created_at\", y=\"author_id\", linestyle=windows[w][0], marker=windows[w][1])\n",
    "            sns.despine()\n",
    "            plt.grid(axis='both', color='grey', linestyle='dashed', linewidth=0.5)\n",
    "            plt.xlabel(\"\")\n",
    "            plt.ylabel(\"\")\n",
    "            plt.ylim(20,100)\n",
    "        plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check activity (number of tweets) per weekday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if process:\n",
    "\n",
    "    # For each network\n",
    "    for net in networks:\n",
    "\n",
    "        print(f\"Network: {net.upper()}\")\n",
    "        # Use a copy of the tweets dataset\n",
    "        temp = networks[net][\"tweets\"].copy()\n",
    "        temp[\"created_at\"] = pd.to_datetime(temp[\"created_at\"])\n",
    "        # Extract the weekday number and name for the dates\n",
    "        temp[\"#\"] = temp[\"created_at\"].apply(lambda x: x.weekday())\n",
    "        temp[\"Day of Week\"] = temp[\"created_at\"].apply(lambda x: x.day_name())\n",
    "\n",
    "        # Calculate the number of tweets posted per day of the week\n",
    "        day_activity = temp.groupby([\"#\", \"Day of Week\"]).size().reset_index()\n",
    "        day_activity.columns = [\"#\", \"Week Day\", \"Activity\"]\n",
    "        # Calculate the percentage of activity\n",
    "        day_activity[\"%\"] = round(day_activity.Activity / day_activity.Activity.sum() * 100,2)\n",
    "\n",
    "        # Plot settings\n",
    "        sns.set(rc={'figure.figsize':(16,4)})\n",
    "        sns.set_style(\"ticks\")\n",
    "        sns.lineplot(day_activity, x=\"Week Day\", y=\"%\", palette=[\"blue\", \"orange\", \"green\"], marker=\"s\")\n",
    "        sns.despine()\n",
    "        plt.grid(axis='y', color='grey', linestyle='dashed', linewidth=0.5)\n",
    "        plt.xlabel(\"Weekday\")\n",
    "        plt.ylabel(\"% Activity\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove users with large inactivity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to remove those users that are not active enough to be considered in the experiment. For this, a baseline threshold is defined:\n",
    "* Window: 7 days. To be considered active, we'll evaluate the amount of tweets the user has in a period of seven days.\n",
    "* Activity Threshold: 4 tweets. In the period of 7 days, the user needs to have tweeted at least 4 times.\n",
    "* Max Gap: 1. If the user doesn't reach this minimum activity threshold in one given week, the \"expressed emotion\" can be interpolated. If the user violates this minimum activity baseline for 2 or more consecutive weeks, then the user is removed from the experiment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters to consider minimum activity of users\n",
    "window = \"7d\"               #-> The window for considering the emotional status of a node\n",
    "activity_threshold = 4      #-> The minimum number of tweets an user needs to be considered \"active\" during the time window\n",
    "max_gap = 1                 #-> The maximum consecutive gap allowed for users to be \"inactive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if process:\n",
    "    \n",
    "    for net in networks:\n",
    "\n",
    "        # We need to convert to Datetime format to truncate miliseconds\n",
    "        networks[net][\"tweets\"].created_at = pd.to_datetime(networks[net][\"tweets\"].created_at, format=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        # Get subset of active users\n",
    "        act_urs = getActiveUsers(networks[net][\"tweets\"], window, max_gap, activity_threshold)\n",
    "        networks[net][\"user_info\"] = networks[net][\"user_info\"].loc[networks[net][\"user_info\"].id.isin(act_urs)]\n",
    "        networks[net][\"follows\"] = networks[net][\"follows\"].loc[networks[net][\"follows\"].mainUserID.isin(act_urs)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Network\n",
    "\n",
    "- Remove users not following each other\n",
    "- Remove nodes not in largest component\n",
    "- Compute edge weights\n",
    "- Create edgelist and graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each network\n",
    "for net in networks:\n",
    "    \n",
    "    if process:\n",
    "\n",
    "            # Use edgeListMaker to create the Edgelist from users following each other\n",
    "            edgelist = edgeListMaker(networks[net][\"user_info\"], networks[net][\"follows\"], networks[net][\"tweets\"], save_edgelist=True, \\\n",
    "            edgelist_path=processed_path + f\"EDGELIST-{net}-undirected.csv\")\n",
    "            networks[net][\"edgelist\"] = edgelist\n",
    "            # We create the Graph from the Edgelist\n",
    "            G = nx.from_pandas_edgelist(edgelist)\n",
    "            networks[net][\"graph\"] = G\n",
    "\n",
    "            # We identify the nodes to reduce the datasets to only those nodes, and store them\n",
    "            nodes = list(G.nodes())\n",
    "            networks[net][\"user_info\"] = networks[net][\"user_info\"].loc[networks[net][\"user_info\"].id.isin(nodes)]\n",
    "            networks[net][\"follows\"] = networks[net][\"follows\"].loc[networks[net][\"follows\"].mainUserID.isin(nodes)]\n",
    "            networks[net][\"tweets\"] = networks[net][\"tweets\"].loc[networks[net][\"tweets\"].author_id.isin(nodes)]\n",
    "        \n",
    "    # Else read from processed data\n",
    "    else:\n",
    "        networks[net][\"graph\"] = nx.read_edgelist(processed_path + f\"EDGELIST-{net}-undirected.csv\", comments=\"source\", delimiter=\",\", data=((\"weight\", float),), encoding='utf-8')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Force all members of US Congress to adopt a bipartisan disposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if access_to_all:\n",
    "    networks[\"us_congressmen\"][\"user_info\"].loc[networks[\"us_congressmen\"][\"user_info\"][\"party\"] != \"Republican\", \"party\"] = \"Democratic\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save reduced process datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if process:\n",
    "\n",
    "    for net in networks:\n",
    "\n",
    "        ## TWEETS ##\n",
    "        # We only need these columns\n",
    "        keep_cols = [\"author_id\", \"id\", \"text\", \"created_at\", \"retweeted\"]\n",
    "        # We only need to know if the tweet is a retweet (transform to bool to save space)\n",
    "        networks[net][\"tweets\"][\"retweeted\"] = networks[net][\"tweets\"][\"retweeted\"].apply(lambda x: False if type(x) != str else True)\n",
    "        networks[net][\"tweets\"] = networks[net][\"tweets\"][keep_cols]\n",
    "        # Save to Processed Data\n",
    "        networks[net][\"tweets\"].to_csv(processed_path + f\"DF-{net}-tweets.csv\", encoding=\"utf-8\", index=False)\n",
    "\n",
    "        ## USER INFO ##\n",
    "        networks[net][\"user_info\"].drop([\"created_at\", \"protected\", \"verified\", \"followers_count\", \"following_count\", \"listed_count\"], axis=1, inplace=True)\n",
    "        # Save to Processed Data\n",
    "        networks[net][\"user_info\"].to_csv(processed_path + f\"DF-{net}-user_info.csv\", encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show Networks basic properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition for different networks\n",
    "partition = {\n",
    "    \"us_congressmen\": \"party\",\n",
    "    \"journalists\": \"newspaper\",\n",
    "    \"universities\": \"university\"\n",
    "    }\n",
    "if access_to_all:\n",
    "    for net in networks:\n",
    "        print(f\"Network -> {net}\")\n",
    "        # Extract list of nodes\n",
    "        nodes = list(networks[net][\"graph\"].nodes())\n",
    "        communities = []\n",
    "        # Construct the set of communities given their respective partition\n",
    "        for p in networks[net][\"user_info\"][partition[net]].unique():\n",
    "            # Nodes for a given partition of the network\n",
    "            p_nodes = networks[net][\"user_info\"].loc[(networks[net][\"user_info\"][partition[net]] == p) & (networks[net][\"user_info\"][\"id\"].isin(nodes))].id\n",
    "            # Append the partition set to the list of partitions\n",
    "            communities.append(set(p_nodes))\n",
    "\n",
    "        GraphProperties(networks[net][\"graph\"], communities=communities)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emotion Recognition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chosen NLP model to extract the emotion from tweets is [TweetNLP](https://tweetnlp.org/), a roBERTa-base model trained on tweets, and further fine-tuned for the task of emotion extraction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the TweetNLP Training set\n",
    "dataset, label_to_id = tweetnlp.load_dataset(\"emotion\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label distributions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For train set in TweetNLP Baseline fine tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look the label distribution\n",
    "total = len(dataset['train']['label'])\n",
    "print(f\"Total training samples: {total}\\n\")\n",
    "print(\"Training dataset label distribution:\\n\")\n",
    "\n",
    "for label, i in label_to_id.items():\n",
    "    nr = len([x for x in dataset[\"train\"][\"label\"] if x == i])\n",
    "    print(f\"{label.upper()}: {nr} ({nr/total:.2%})\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For annotation samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if access_to_all:\n",
    "    # Let's load our annotations\n",
    "    samples, labels = load_annotations(finetuning_path + '/annotations/')\n",
    "\n",
    "    # Let's look the label distribution for our annotations\n",
    "    total = len(labels)\n",
    "    print(f\"Total annotated samples: {total}\\n\")\n",
    "    print(\"Annotated dataset label distribution:\\n\")\n",
    "\n",
    "    for label in [\"undefined\"] + list(label_to_id.keys()):\n",
    "        nr = len([x for x in labels if x == label])\n",
    "        print(f\"{label.upper()}: {nr} ({nr/total:.2%})\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label distribution of samples used in fine tuning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if access_to_all:\n",
    "    # Let's look the label distribution for our custom fine tuning\n",
    "    train = pd.read_csv(finetuning_path + '/all/train.csv', encoding=\"utf-8\")\n",
    "    total = train.shape[0]\n",
    "    print(f\"Total training samples: {total}\\n\")\n",
    "    print(\"Training dataset label distribution:\\n\")\n",
    "\n",
    "    for label, i in label_to_id.items():\n",
    "        nr = train[train.label == i].shape[0]\n",
    "        print(f\"{label.upper()}: {nr} ({nr/total:.2%})\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classify using Fine Tuned TweetNLP\n",
    "\n",
    "Given the large amount of tweets to classify (2.060.983), this stage was performed in ITU's High Performance Cluster for approximately 206 hs. <br>\n",
    "The example set up follows here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if access_to_all:\n",
    "    if classify:\n",
    "    \n",
    "    # Load Finetuned model\n",
    "    model = tweetnlp.load_model('emotion', model_name=\"./nlp_model/models/all\")\n",
    "\n",
    "    # Set a batch size according to resources\n",
    "    batch_size = 10000\n",
    "\n",
    "    # Classify tweets for each network\n",
    "    for net in networks:\n",
    "    \n",
    "        # We need to split tweet dataset in batches\n",
    "        classified_samples = []\n",
    "\n",
    "        # Read tweet dataset\n",
    "        tweets = pd.read_csv(f\"./data/1- Processed data/DF-{net}-tweets.csv\", encoding=\"utf-8\")\n",
    "\n",
    "        # Get number of batches needed\n",
    "        batches =  tweets.shape[0] // batch_size + 1\n",
    "\n",
    "        # Predict by batches\n",
    "        for batch in range(0, batches):\n",
    "\n",
    "            # If it's the last batch, slice to the end\n",
    "            if batch == batches-1:\n",
    "                tweets_subset = tweets.iloc[(batch*batch_size):]\n",
    "\n",
    "            # Otherwise slice to the batch\n",
    "            else:\n",
    "                tweets_subset = tweets.iloc[batch*batch_size:(batch*batch_size)+batch_size]\n",
    "\n",
    "            # Predict on text\n",
    "            result = model.predict(tweets_subset.text, return_probability = True)\n",
    "            result = pd.DataFrame(result)\n",
    "\n",
    "            # Extract the predictions and calculate the entropy\n",
    "            joy = []\n",
    "            optimism = []\n",
    "            anger = []\n",
    "            sadness = []\n",
    "\n",
    "            for ix, row in result.iterrows():\n",
    "                joy.append(row[\"probability\"][\"joy\"])\n",
    "                optimism.append(row[\"probability\"][\"optimism\"])\n",
    "                anger.append(row[\"probability\"][\"anger\"])\n",
    "                sadness.append(row[\"probability\"][\"sadness\"])\n",
    "\n",
    "            result[\"joy\"] = joy\n",
    "            result[\"optimism\"] = optimism\n",
    "            result[\"anger\"] = anger\n",
    "            result[\"sadness\"] = sadness\n",
    "            result.drop(\"probability\", axis=1, inplace=True)\n",
    "            result[\"nEntropy\"] = result[[\"joy\", \"optimism\", \"anger\", \"sadness\"]].apply(entropy, axis=1)\n",
    "            result[\"nEntropy\"] = result.nEntropy / (np.log(4))\n",
    "\n",
    "            # Append the batch results\n",
    "            classified_samples.append(result)\n",
    "\n",
    "        # Concat all results in one DF\n",
    "        classified_samples = pd.concat(classified_samples)\n",
    "\n",
    "        # Keep only User ID, Creation Date and Classification results\n",
    "        classified_samples = classified_samples[[\"author_id\", \"created_at\", \"retweeted\", \"joy\", \"optimism\", \"anger\", \"sadness\", \"nEntropy\"]]\n",
    "\n",
    "        # Save to file\n",
    "        classified_samples.to_csv(f\"{emotions_path}{net}-emotions.csv\", encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now load the classified emotions per tweet, user and date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for net in networks:\n",
    "\n",
    "    networks[net][\"emotions\"] = pd.read_csv(f\"{emotions_path}{net}-tweets-emotions.csv\", encoding=\"utf-8\", dtype={\"author_id\": str})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform uncertain predictions into undefined, aggregate emotions and interpolate missing emotions in gaps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First, let's look at the proportion of labeled samples given our entropy cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decided entropy cutoff\n",
    "entropy_cutoff = 0.07596\n",
    "\n",
    "print(f\"With entropy cutoff at {entropy_cutoff}\")\n",
    "for net in networks:\n",
    "    #np.percentile(networks[net][\"emotions\"].nEntropy, np.linspace(0,100,11))\n",
    "    prop = networks[net][\"emotions\"][networks[net][\"emotions\"].nEntropy <= entropy_cutoff].shape[0] /networks[net][\"emotions\"].shape[0]\n",
    "    print(f\"Network {net}: we keep {prop:.2%} labeled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for net in networks:\n",
    "\n",
    "    # Scrap values from undefined emotions, given an entropy threashold\n",
    "    networks[net][\"emotions-undefined\"] = undefine(networks[net][\"emotions\"], entropy = entropy_cutoff)\n",
    "\n",
    "    # Remove noise from low probabilities\n",
    "    networks[net][\"emotions-undefined\"] = remove_noise(networks[net][\"emotions-undefined\"])\n",
    "\n",
    "    # Aggregate the emotions by week\n",
    "    networks[net][\"emotions-aggregated\"] = find_weekly_values(networks[net][\"emotions-undefined\"])\n",
    "    networks[net][\"emotions-aggregated\"].rename(columns={\"agg_optimism\": \"optimism\", \"agg_joy\": \"joy\", \"agg_anger\": \"anger\", \"agg_sadness\": \"sadness\"}, inplace=True)\n",
    "\n",
    "    # Interpolate the gaps\n",
    "    dfs = []\n",
    "    for author in networks[net][\"emotions-aggregated\"].author_id.unique():\n",
    "\n",
    "        df = networks[net][\"emotions-aggregated\"].loc[networks[net][\"emotions-aggregated\"].author_id == author]\n",
    "        df = interpolate_values(df, activity_threshold = activity_threshold, emotion_idx = [2,3,4,5], act_idx = 6)\n",
    "        dfs.append(df)\n",
    "\n",
    "    networks[net][\"emotions-interpolated\"] = pd.concat(dfs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing for interpolation accuracy\n",
    "To ensure that the interpolation process has not changed the nature of the data, we compute a few statistics:\n",
    "- Compare average emotion fluctuationa between time steps for all users in all networks\n",
    "- Evaluate if the distributions are different with a KS test\n",
    "- Calculate MAE, RMSE and R2 for data before and after interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the emotion fluctuation between timesteps for all users\n",
    "before_avg = []\n",
    "after_avg = []\n",
    "\n",
    "for net in networks:\n",
    "\n",
    "    print(f\"{net.upper()}\")\n",
    "\n",
    "    # Keep only continuos week rows\n",
    "    continous = networks[net][\"emotions-aggregated\"][networks[net][\"emotions-aggregated\"][\"weekly_tweet_values\"] >= 4]\n",
    "    # Calculate the gap to ensure we only keep continuos weeks\n",
    "    continous[\"gap\"] = continous.week.diff().fillna(1)\n",
    "    continous = continous[continous.gap < 2]\n",
    "\n",
    "    # For each emotion column\n",
    "    for e in emotions:\n",
    "\n",
    "        # We extract the emotion vector before and after interpolation\n",
    "        original_data = networks[net][\"emotions-aggregated\"][e]\n",
    "        interpolated_data = networks[net][\"emotions-interpolated\"][e]\n",
    "\n",
    "        # We calculate the critical value for the KS test\n",
    "        critical_value = 1.358 / (len(original_data) + len(interpolated_data))**0.5  # for alpha = 0.05\n",
    "        print(f\"{e.capitalize()} (critical value: {critical_value:.3f})\") \n",
    "\n",
    "        # Perform KS test\n",
    "        stat, p_value = ks_2samp(original_data, interpolated_data)\n",
    "\n",
    "        # Calculate metrics on dataset difference \n",
    "        # Mean Absolute Error (MAE)\n",
    "        mae = np.mean(np.abs(interpolated_data - original_data))\n",
    "        # Root Mean Squared Error (RMSE)\n",
    "        rmse = np.sqrt(np.mean((interpolated_data - original_data) ** 2))\n",
    "        # Coefficient of Determination (R-squared)\n",
    "        total_variation = np.sum((original_data - np.mean(original_data)) ** 2)\n",
    "        residual_variation = np.sum((interpolated_data - original_data) ** 2)\n",
    "        r_squared = 1 - (residual_variation / total_variation)\n",
    "\n",
    "        # compare p-value to significance level (e.g., 0.05)\n",
    "        if stat > critical_value:\n",
    "            print(f\"SAME DISTRIBUTION (stat: {stat:.3f} - p-value: {p_value:.3f})\")\n",
    "        else:\n",
    "            print(f\"DIFFERENT DISTRIBUTION (stat: {stat:.3f} - p-value: {p_value:.3f})\")\n",
    "\n",
    "        print(f\"Mean Absolute Error: {mae:.3f}\")\n",
    "        print(f\"Root Mean Squared Error: {rmse:.3f}\")\n",
    "        print(f\"R-squared: {r_squared:.3f}\")\n",
    "    \n",
    "        # For each author separately\n",
    "        for a in continous.author_id.unique():\n",
    "            # Calculate the fluctuation before and after interpolation\n",
    "            before_interpolation = continous.loc[continous.author_id == a, e].diff().fillna(0)\n",
    "            after_interpolation = networks[net][\"emotions-interpolated\"].loc[networks[net][\"emotions-interpolated\"].author_id == a, e].diff().fillna(0)\n",
    "            # Calculate their emotion fluctuation\n",
    "            before_avg.append(sum(abs(before_interpolation)) / before_interpolation.shape[0])\n",
    "            after_avg.append(sum(abs(after_interpolation)) / after_interpolation.shape[0])\n",
    "\n",
    "print(f\"Before interpolation, the avg. fluctuation for all users in all networks is {np.mean(before_avg):.5f}\")\n",
    "print(f\"After interpolation, the avg. fluctuation for all users in all networks is {np.mean(after_avg):.5f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emotion Presence in the Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each network\n",
    "for net in networks:\n",
    "\n",
    "    print(net.upper())\n",
    "\n",
    "    # Melt emotion columns to show boxplots\n",
    "    temp = networks[net][\"emotions-interpolated\"].melt(value_vars=[\"optimism\", \"joy\", \"anger\", \"sadness\"])\n",
    "\n",
    "    # Plot settings\n",
    "    sns.set(rc={'figure.figsize':(16,2)})\n",
    "    sns.set_style(\"ticks\")\n",
    "    sns.boxplot(data = temp, y=\"variable\", x=\"value\", palette=emotions)\n",
    "    plt.xlim(-0.01,1.01)\n",
    "    plt.xlabel(\"\")\n",
    "    plt.ylabel(\"\")\n",
    "    plt.grid(axis=\"x\", linestyle=\"dotted\", color=\"grey\", linewidth=0.5)\n",
    "    sns.despine()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Emotion presence avg, std, and quartiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for net in networks:\n",
    "    print(f\"NETWORK {net.upper()}\")\n",
    "    temp = networks[net][\"emotions-interpolated\"][emotions].describe()\n",
    "    temp.drop(\"count\", inplace=True)\n",
    "    display(temp.T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing for significant differences between the networks\n",
    "We first use a Levene test to see if the data sets are suitable for an ANOVA test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for net in networks:\n",
    "    print(f\"Network {net.upper()}\")\n",
    "    for e in emotions:\n",
    "        stat, p_value = kruskal(*[networks[net]['emotions-interpolated'][e] for net in networks])\n",
    "        print(f\"Emotion {e.capitalize()} - P-Value: {p_value} - {p_value / (0.05 * 4)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at emotions per network sub-groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if access_to_all:\n",
    "    # Since the groups in each network are called something different, they are coded seperatly\n",
    "    networks['us_congressmen']['Group'] = (networks['us_congressmen']['emotions-interpolated'].join(\n",
    "        networks['us_congressmen']['user_info'][['id', 'party']].set_index('id'), on = 'author_id').rename(columns = {'party' : 'Group'}))\n",
    "\n",
    "    networks['journalists']['Group'] = (networks['journalists']['emotions-interpolated'].join(\n",
    "        networks['journalists']['user_info'][['id', 'newspaper']].set_index('id'), on = 'author_id').rename(columns = {'newspaper' : 'Group'}))\n",
    "\n",
    "    networks['universities']['Group'] = (networks['universities']['emotions-interpolated'].join(\n",
    "        networks['universities']['user_info'][['id', 'university']].set_index('id'), on = 'author_id').rename(columns = {'university' : 'Group'}))\n",
    "\n",
    "    for net in networks:\n",
    "        networks[net]['sub_group'] = pd.melt(networks[net]['Group'].groupby(['week', 'Group']).mean().reset_index(),\n",
    "            id_vars=['week', 'Group'], value_vars=[\"optimism\", \"joy\", \"anger\", \"sadness\"])\n",
    "\n",
    "    for net in networks:\n",
    "        networks[net]['group_dict'] = {}\n",
    "        for group in networks[net]['sub_group']['Group']:\n",
    "            networks[net]['group_dict'][group] = networks[net]['Group'].author_id[networks[net]['Group'].Group == group]\n",
    "        networks[net].pop(\"Group\")\n",
    "        networks[net].pop(\"group_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if access_to_all:\n",
    "    sns.set_style('ticks')\n",
    "    sns.set(rc={'figure.figsize':(16,4)})\n",
    "\n",
    "    for net in networks:\n",
    "        sns.barplot(data = networks[net]['sub_group'], x = 'variable', y = 'value', hue = 'Group', palette='rocket')\n",
    "        plt.grid(axis = 'y', linestyle = 'dotted', color = 'gray', linewidth = 0.5)\n",
    "        sns.despine()\n",
    "        plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Significance test on the mean difference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test if there is a significant difference between the sub-groups within each network, we compute the mean emotional state for users within the same sub-group for all weeks. Since the variance is significantly different within groups, we perform a Kruskal-Wallis test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if access_to_all:\n",
    "    print(\"KRUSKAL-WILLIS - TEST\")\n",
    "    for net in networks:\n",
    "        for e in emotions:\n",
    "            temp = networks[net]['sub_group'][networks[net]['sub_group']['variable'] == e]\n",
    "            stat, p_value = kruskal(*[temp['value'] [temp.Group == group] for group in temp.Group.unique()])\n",
    "        print(f\"For network {net.upper()}: P-Value: {p_value:.5f}\")\n",
    "\n",
    "    print(\"\\nTUKEY's HSD - TEST\")\n",
    "\n",
    "    for net in networks:\n",
    "        print(f\"Network {net.upper()}\")\n",
    "        for e in emotions:\n",
    "            temp = networks[net]['sub_group'][networks[net]['sub_group']['variable'] == e]\n",
    "            res = tukey_hsd(*[temp['value'] [temp.Group == group] for group in temp.Group.unique()])\n",
    "            p_value = res.pvalue.flatten()[res.statistic.argmax()]\n",
    "            print(f\"{e.capitalize()} - P-Value: {p_value:.5f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Propagation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare networks:\n",
    "- Calculate the inverse of the Laplacian matrix\n",
    "- Melt the emotion columns to one 'Emotion' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if propagate:\n",
    "    for net in networks:\n",
    "\n",
    "        # Needed graph calculations\n",
    "        Q = nx.adjacency_matrix(networks[net][\"graph\"], weight = \"weight\")\n",
    "        Q = csgraph.laplacian(Q, normed = False)\n",
    "        Q = Q.todense()\n",
    "        Q = Q.astype(float)\n",
    "        Q = np.linalg.pinv(Q)\n",
    "\n",
    "        networks[net][\"Q\"] = Q\n",
    "\n",
    "        df = pd.melt(networks[net][\"emotions-interpolated\"], id_vars=['author_id', 'week'], value_vars=['joy', 'anger', 'sadness', 'optimism'], var_name=\"emotion\")\n",
    "\n",
    "        # List of users and weeks with amount of emotion\n",
    "        networks[net][\"emotions-weekly\"] = df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate distances for emotions between weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks = 51\n",
    "\n",
    "for net in networks:\n",
    "\n",
    "   if propagate:\n",
    "\n",
    "      # List to store distances\n",
    "      distances = []\n",
    "      distances_norm = []\n",
    "\n",
    "      # For each emotion\n",
    "      for emotion in networks[net][\"emotions-weekly\"].emotion.unique():\n",
    "\n",
    "         # Isolate the emotion\n",
    "         df_emotion = networks[net][\"emotions-weekly\"][networks[net][\"emotions-weekly\"][\"emotion\"] == emotion]\n",
    "\n",
    "         week_array = []\n",
    "         # For each week\n",
    "         for w in range(weeks):\n",
    "            # Isolate emotions for a given week\n",
    "            df_week = df_emotion[df_emotion.week == w]\n",
    "            _ = {}\n",
    "            # For each node in graph\n",
    "            for node in networks[net][\"graph\"].nodes:\n",
    "               # If node is in the week subset\n",
    "               if node in list(df_week.author_id.unique()):\n",
    "                  # Find the value for a given node in a given week, in the form \"node\": value (of emotion)\n",
    "                  _[node] = df_week.loc[(df_week.author_id == node), \"value\"].values[0]\n",
    "            # Append to the week array\n",
    "            week_array.append(_)\n",
    "\n",
    "         # Calculate distances from week to week\n",
    "         for _ in range(len(week_array) - 1):\n",
    "\n",
    "            distances.append((emotion, _, _ + 1, nd.ge(week_array[_], week_array[_ + 1], networks[net][\"graph\"], Q = networks[net][\"Q\"], normed = False)))\n",
    "            distances_norm.append((emotion, _, _ + 1, nd.ge(week_array[_], week_array[_ + 1], networks[net][\"graph\"], Q = networks[net][\"Q\"], normed = True)))\n",
    "\n",
    "      distances = pd.DataFrame(distances, columns=[\"Emotion\", \"From\", \"To\", \"Distance\"])\n",
    "      distances_norm = pd.DataFrame(distances_norm, columns=[\"Emotion\", \"From\", \"To\", \"Norm. Distance\"])\n",
    "      df = distances.merge(distances_norm, on=[\"Emotion\", \"From\", \"To\"], how=\"inner\")\n",
    "\n",
    "      networks[net][\"distances\"] = df\n",
    "\n",
    "      # Save to disk\n",
    "      networks[net][\"distances\"].to_csv(f\"{propagation_path}{net}-distances.csv\", encoding=\"utf-8\", index=False)\n",
    "      \n",
    "   # Else we load the Propagation Data\n",
    "   else:\n",
    "      networks[net][\"distances\"] = pd.read_csv(f\"{propagation_path}{net}-distances.csv\", encoding=\"utf-8\", dtype={\"Distance\": float})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results per week"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-normalized"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not being normalized, the propagation shows both structural changes in the network as well as propagation among nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for net in networks:\n",
    "    sns.set(rc={'figure.figsize':(16,4)})\n",
    "    sns.set_style(\"ticks\")\n",
    "    ax = sns.lineplot(data= networks[net][\"distances\"], x= \"To\", y= \"Distance\", hue= \"Emotion\", palette=emotions)\n",
    "    plt.xlim(0)\n",
    "    plt.ylim(0)\n",
    "    plt.xlabel(\"Week\")\n",
    "    plt.grid(\"both\", linestyle=\"dotted\", color=\"grey\", linewidth=0.5)\n",
    "    sns.despine()\n",
    "    plt.title(net)\n",
    "    plt.legend(\"\")\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emotion Presence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each network\n",
    "for net in networks:\n",
    "\n",
    "    print(net.upper())\n",
    "\n",
    "    # Melt emotion columns to show boxplots\n",
    "    temp = networks[net][\"emotions-interpolated\"].melt(value_vars=[\"optimism\", \"joy\", \"anger\", \"sadness\"])\n",
    "    # Plot settings\n",
    "    sns.set(rc={'figure.figsize':(16,2)})\n",
    "    sns.set_style(\"ticks\")\n",
    "    sns.boxplot(data = temp, y=\"variable\", x=\"value\", palette=emotions)\n",
    "    plt.xlim(-0.01,1.01)\n",
    "    plt.xlabel(\"\")\n",
    "    plt.ylabel(\"\")\n",
    "    plt.grid(axis=\"x\", linestyle=\"dotted\", color=\"grey\", linewidth=0.5)\n",
    "    sns.despine()\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Emotion presence avg, std, and quartiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for net in networks:\n",
    "    print(f\"NETWORK {net.upper()}\")\n",
    "    temp = networks[net][\"emotions-interpolated\"][emotions].describe()\n",
    "    temp.drop(\"count\", inplace=True)\n",
    "    display(temp.T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing for significant diffirences between the networks\n",
    "We first use a Levene test to see if the data sets are suitable for an ANOVA test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for net in networks:\n",
    "    print(f\"Network {net.upper()}\")\n",
    "    for e in emotions:\n",
    "        stat, p_value =kruskal(*[networks[net]['emotions-interpolated'][e] for net in networks])\n",
    "        print(f\"Emotion {e.capitalize()} - P-Value: {p_value} - {p_value / (0.05 * 4)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distances per Network - Normalized"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By being normalized, the propagation shows only the propagation proportionally among nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for net in networks.keys():\n",
    "    sns.set(rc={'figure.figsize':(16,4)})\n",
    "    sns.set_style(\"ticks\")\n",
    "    sns.lineplot(data= networks[net][\"distances\"], x= \"To\", y= \"Norm. Distance\", hue= \"Emotion\", palette=emotions)\n",
    "    plt.xlim(0)\n",
    "    plt.ylim(0)\n",
    "    plt.legend(\"\")\n",
    "    plt.xlabel(\"\")\n",
    "    plt.grid(\"both\", linestyle=\"dotted\", color=\"grey\", linewidth=0.5)\n",
    "    sns.despine()\n",
    "    plt.title(net)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Propagation speed per network, avg. and std. for both Normalized and non-Normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for net in networks:\n",
    "    print(f\"NETWORK {net.upper()}\")\n",
    "    display(networks[net][\"distances\"].loc[:,[\"Emotion\", \"Distance\", \"Norm. Distance\"]].groupby(\"Emotion\").agg([\"mean\", \"std\"]).reset_index().\\\n",
    "            sort_values((\"Norm. Distance\", \"std\"), ascending = False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance Scoreboard (top 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for net in networks:\n",
    "    print(net.upper())\n",
    "    display(networks[net][\"distances\"].sort_values(by = \"Norm. Distance\", ascending = False).head(10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emotion Acceleration (delta Speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for net in networks:\n",
    "\n",
    "    speed_changes = networks[net][\"distances\"].copy()\n",
    "    dfs = []\n",
    "    for e in emotions:\n",
    "        _ = speed_changes.loc[speed_changes.Emotion == e]\n",
    "        _[\"change\"] = _[\"Norm. Distance\"].diff()\n",
    "        dfs.append(_)\n",
    "    speed_changes = pd.concat(dfs)\n",
    "\n",
    "    sns.set(rc={'figure.figsize':(16,4)})\n",
    "    sns.set_style(\"ticks\")\n",
    "    ax = sns.lineplot(data= speed_changes, x= \"To\", y= \"change\", hue= \"Emotion\", palette=emotions)\n",
    "    plt.xlim(0)\n",
    "    plt.xlabel(\"Week\")\n",
    "    plt.ylabel(\"Acceleration\")\n",
    "    plt.legend(\"\")\n",
    "    plt.grid(\"both\", linestyle=\"dotted\", color=\"grey\", linewidth=0.5)\n",
    "    sns.despine()\n",
    "    plt.title(net)\n",
    "    plt.show()\n",
    "\n",
    "    print(net)\n",
    "    display(speed_changes.sort_values(\"change\", ascending=False).head(10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation between emotion presence and emotion variance against emotion propagation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotion presence vs. Emotion Distance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look at the correlation between the presence of emotions at the begining of the week and the propagation between that week and the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"EMOTION PRESENCE AT T AND DISTANCE FROM T - T+1\")\n",
    "\n",
    "for net in networks:\n",
    "\n",
    "    print(f\"\\nNetwork: {net.upper()}\")\n",
    "\n",
    "    for e in emotions:\n",
    "\n",
    "        emo_vector = networks[net][\"emotions-interpolated\"].groupby(\"week\").sum().reset_index().loc[:49,e]\n",
    "        emo_distance = networks[net][\"distances\"][networks[net][\"distances\"].Emotion == e][\"Distance\"]\n",
    "        corr, pvalue = pearsonr(emo_vector, emo_distance)\n",
    "\n",
    "        print(f\"{e.capitalize()}: {corr:.3f} - P-Value: {pvalue:.5f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotion variance vs. Emotion Distance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look at the correlation between the variance of emotions from week to week and the propagation between those weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"EMOTION DISTANCE FROM T - T+1 AND ITS DISTANCE\")\n",
    "\n",
    "for net in networks:\n",
    "\n",
    "    print(f\"\\nNetwork: {net.upper()}\")\n",
    "\n",
    "    for e in emotions:\n",
    "\n",
    "        emo_diff = networks[net][\"emotions-interpolated\"].groupby(\"week\").mean()[e].diff().dropna()\n",
    "        emo_distance = networks[net][\"distances\"][networks[net][\"distances\"].Emotion == e][\"Distance\"]\n",
    "        corr, pvalue = pearsonr(emo_diff, emo_distance)\n",
    "\n",
    "        print(f\"{e.capitalize()}: {corr:.3f} - P-Value: {pvalue:.5f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spearman correlation between week number and emotion presence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wish to inspect if there is a general trend in emotion presence throughout the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.05 / 12\n",
    "\n",
    "for net in networks:\n",
    "    for e in emotions:\n",
    "        c, p = pearsonr([i for i in range(51)], networks[net][\"emotions-interpolated\"].groupby(\"week\").mean()[e])\n",
    "        print(f\"{net.upper()} - {e} - Statistic: {c:.3f}, p-value: {p:.5f} - {p < threshold}\")\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Results & Resources"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Avg. and Std. of speeds for each network - All weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for net in networks:\n",
    "    print(net.upper())\n",
    "    display(networks[net][\"distances\"][[\"Emotion\", \"Distance\", \"Norm. Distance\"]].groupby(\"Emotion\").mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of propagation in small network:\n",
    "- 5 Time steps\n",
    "- 9 Nodes\n",
    "- No edge weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examining the peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network to examine\n",
    "net = \"us_congressmen\"\n",
    "# Week to look at\n",
    "# None if all period\n",
    "w = None\n",
    "# Emotion to look at\n",
    "# None if all emotions\n",
    "e = None\n",
    "\n",
    "if w != None:\n",
    "    begin= (pd.to_datetime(\"2022-01-01\")) + timedelta(7*w)\n",
    "    end = (pd.to_datetime(\"2022-01-01\")) + timedelta(7*(w+1)-1)\n",
    "else:\n",
    "    begin = pd.to_datetime(\"2022-01-01\")\n",
    "    end = pd.to_datetime(\"2022-12-23\")\n",
    "\n",
    "print(f\"We are looking at:\\n{net.upper()}\\n{e if e != None else 'All emotions'}\\nEvents between {begin} and {end}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numbers for the peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if access_to_all:\n",
    "    see_sub_grup_weekly_change(networks['universities']['sub_group'], 14, 'anger', 2, net = 'universities')\n",
    "    see_sub_grup_weekly_change(networks['universities']['sub_group'], 32, 'anger', 2, net = 'universities')\n",
    "    see_sub_grup_weekly_change(networks['us_congressmen']['sub_group'], 36, 'optimism', 3, net = 'us_congressmen')\n",
    "    see_sub_grup_weekly_change(networks['journalists']['sub_group'], 35, 'optimism', 2, net = 'journalists')\n",
    "    see_sub_grup_weekly_change(networks['journalists']['sub_group'], 35, 'optimism', 2, net = 'journalists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_change(odf, emotions):\n",
    "    \"\"\"Find the mean change across the network each day for each feeling.\"\"\"\n",
    "    df = odf.copy()\n",
    "    df = df.sort_values(['author_id', 'week'])\n",
    "\n",
    "    for emotion in emotions:\n",
    "\n",
    "        df[emotion + '_change'] = df[emotion].diff().abs()\n",
    "\n",
    "    return df[df.week != 0]\n",
    "\n",
    "\n",
    "def absolute_change(net, emotion_order):\n",
    "    \"\"\"Find the absolute change across the network each day for each feeling.\"\"\"\n",
    "    overlap = [i % 51 != 0 for i in range(int(len(net)/4))][:-1]\n",
    "    overlap[0] = True\n",
    "\n",
    "    values = net.set_index(['author_id', 'emotion', 'week'])\n",
    "    values.sort_index()\n",
    "\n",
    "\n",
    "    moved = values.diff().abs().reset_index()\n",
    "    moved = moved[moved['week'] != 50]\n",
    "    final = moved.groupby(['emotion', 'week']).value.sum()\n",
    "    final = final.reset_index()\n",
    "    emotions = []\n",
    "    for emotion in emotion_order:\n",
    "        emotions.extend(final[final.emotion == emotion].value)\n",
    "\n",
    "    return emotions\n",
    "\n",
    "mean_change = mean_change(networks['us_congressmen']['emotions-interpolated'], list(emotions.keys()))\n",
    "mean_changes = mean_change[list(emotions.keys()) + ['week']].groupby('week').mean()\n",
    "print(\"The mean change for congressmen for week 36 is\", mean_changes['optimism'][36])\n",
    "print(\"The mean change for congressmen for all weeks is\", mean_change['optimism'].mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for inactivity in all networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for net in networks.keys():\n",
    "    no_tweets = networks[net]['emotions-aggregated'][networks[net]['emotions-aggregated'].weekly_tweet_values == 0]\n",
    "    low_tweets = networks[net]['emotions-aggregated'][networks[net]['emotions-aggregated'].weekly_tweet_values < 4]\n",
    "    print(f\"{net} is inactive in {len(no_tweets) / len(networks[net]['emotions-aggregated']) * 100}% of weeks\")\n",
    "    print(f\"{net} have low activity in {len(low_tweets) / len(networks[net]['emotions-aggregated']) * 100}% of weeks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "4ba4a03950c7e0d71404c609bc7b34a08ca1d9205d54330cb0330faeb21bfe20"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
